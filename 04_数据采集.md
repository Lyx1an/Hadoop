# 安装数据采集软件Flume

> `前提条件`:
>
> - 业务系统需要有hadoop的客户端

## 安装hadoop集群客户端

直接从hadoop01节点通过scp拷贝客户端到biz01

```shell
# 在biz01上执行
cd  /bigdata/server
scp root@hadoop01:/bigdata/server/hadoop-3.3.3 .
# 创建软链接
ln -s  hadoop-3.3.3  hadoop

# 设置好主机名
vi /etc/hosts

192.168.113.145 hadoop01 hadoop01
192.168.113.146 hadoop02 hadoop02
192.168.113.147 hadoop03 hadoop03
192.168.113.148 biz01 biz01
```

## 安装flume数据采集软件

在biz01上安装flume数据采集软件

```shell
# 1 上传apache-flume-1.10.1-bin.tar.gz 到 /bigdata/soft 目录
# 2 解压到指定目录
tar -zxvf apache-flume-1.10.1-bin.tar.gz -C /bigdata/server/
# 3 创建软链接
cd /bigdata/server
ln -s apache-flume-1.10.1-bin/ flume
```

## 配置环境变量

```shell
vi /etc/profile.d/my_env.sh

#!/bin/bash
#JAVA_HOME
export JAVA_HOME=/bigdata/server/jdk1.8
export PATH=$PATH:$JAVA_HOME/bin

#FLUME_HOME
export FLUME_HOME=/bigdata/server/flume
export PATH=$PATH:$FLUME_HOME/bin

#HADOOP_HOME
export HADOOP_HOME=/bigdata/server/hadoop
export PATH=$PATH:$HADOOP_HOME/bin

# 加载配置文件
source /etc/profile
```

## 测试环境

```shell
# 测试hadoop环境
hdfs dfs -ls /
```

## 配置Flume采集数据

### 在lib目录添加一个ETL拦截器

- 处理标准的json格式的数据, 如果格式不符合条件, 则会过滤掉该信息
- 处理时间漂移的问题, 把对应的日志存放到具体的分区数据中

在业务服务器的Flume的lib目录添加itercepter-etl.jar

### 配置采集数据到hdfs文件的配置

在flume的家目录创建文件 jobs/log_file_to_hdfs.conf

```shell
#为各组件命名
a1.sources = r1
a1.channels = c1
a1.sinks = k1

#描述source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /bigdata/soft/app/log/behavior/.*
a1.sources.r1.positionFile = /bigdata/server/flume/position/behavior/taildir_position.json
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type = cn.wolfcode.flume.interceptor.ETLInterceptor$Builder
a1.sources.r1.interceptors =  i2
a1.sources.r1.interceptors.i2.type = cn.wolfcode.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /behavior/origin/log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = DataStream

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
```

### 运行数据采集命令

```shell
# 进入到Flume的目录
cd  /bigdata/server/flume
```



```shell
bin/flume-ng agent --conf conf/ --name a1 --conf-file jobs/log_file_to_hdfs.conf -Dflume.root.logger=INFO,console 
```



```shell
# 后台启动运行
nohup bin/flume-ng agent --conf conf/ --name a1 --conf-file jobs/log_file_to_hdfs.conf -Dflume.root.logger=INFO,console >/bigdata/server/flume/logs/log_file_to_hdfs.log 2>&1  &
```

### 日志采集效果

<img src="image/image-20220829101315186.png" alt="image-20220829101315186" style="zoom:150%;" />